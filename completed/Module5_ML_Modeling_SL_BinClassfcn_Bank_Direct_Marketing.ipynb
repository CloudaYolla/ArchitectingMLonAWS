{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5 ML Modeling: Binary Classification on Bank Direct Marketing Dataset\n",
    "\n",
    "\n",
    "`(Revision History:\n",
    "PA1, 2020-04-14, @akirmak: Initial version\n",
    "`\n",
    "\n",
    "## Module Overview\n",
    "This notebook is a slightly modified copy of the [AWS SageMaker Samples in Github: xgboost_direct_marketing_sagemaker\n",
    " ](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_applying_machine_learning/xgboost_direct_marketing)\n",
    " \n",
    "In this lab, you will \n",
    " 1. quickly execute the Data Exploration & Data Transformation Steps \n",
    " 1. mainly focus on the Training Step\n",
    " 1. You will also skip the hosting step\n",
    " 1. Discuss with the instructor the 4 Well Architected Principles of ML Training in the cloud with AWS ( scalability, cost optimization, performance & operational excellence impact. Security is excluded in this workshop for simplicity, but it is important to note that Security is Job Zero in a Production grade project) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Targeting Direct Marketing with Amazon SageMaker XGBoost\n",
    "_**Supervised Learning with Gradient Boosted Trees: A Binary Prediction Problem With Unbalanced Classes**_\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Prepration](#Preparation)\n",
    "1. [Data](#Data)\n",
    "    1. [Exploration](#Exploration)\n",
    "    1. [Transformation](#Transformation)\n",
    "1. [Training](#Training)\n",
    "1. [Hosting](#Hosting)\n",
    "1. [Evaluation](#Evaluation)\n",
    "1. [Exentsions](#Extensions)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "Direct marketing, either through mail, email, phone, etc., is a common tactic to acquire customers.  Because resources and a customer's attention is limited, the goal is to only target the subset of prospects who are likely to engage with a specific offer.  Predicting those potential customers based on readily available information like demographics, past interactions, and environmental factors is a common machine learning problem.\n",
    "\n",
    "This notebook presents an example problem to predict if a customer will enroll for a term deposit at a bank, after one or more phone calls.  The steps include:\n",
    "\n",
    "* Preparing your Amazon SageMaker notebook\n",
    "* Downloading data from the internet into Amazon SageMaker\n",
    "* Investigating and transforming the data so that it can be fed to Amazon SageMaker algorithms\n",
    "* Estimating a model using the Gradient Boosting algorithm\n",
    "* Evaluating the effectiveness of the model\n",
    "* Setting the model up to make on-going predictions\n",
    "\n",
    "---\n",
    "\n",
    "## Preparation\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "bucket = 'prj-ml' # eg. prj-ml architecting-ml-aws \n",
    "prefix = 'architecting-ml-aws/mod3-bank-dm'\n",
    " \n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's bring in the Python libraries that we'll use throughout the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                # For matrix operations and numerical processing\n",
    "import pandas as pd                               # For munging tabular data\n",
    "import matplotlib.pyplot as plt                   # For charts and visualizations\n",
    "from IPython.display import Image                 # For displaying images in the notebook\n",
    "from IPython.display import display               # For displaying outputs in the notebook\n",
    "from time import gmtime, strftime                 # For labeling SageMaker models, endpoints, etc.\n",
    "import sys                                        # For writing outputs to notebook\n",
    "import math                                       # For ceiling function\n",
    "import json                                       # For parsing hosting outputs\n",
    "import os                                         # For manipulating filepath names\n",
    "import sagemaker                                  # Amazon SageMaker's Python SDK provides many helper functions\n",
    "from sagemaker.predictor import csv_serializer    # Converts strings for HTTP POST requests on inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data\n",
    "Let's start by downloading the [direct marketing dataset](https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip) from the sample data s3 bucket. \n",
    "\n",
    "\\[Moro et al., 2014\\] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-14 21:21:38--  https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip\n",
      "Resolving sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com (sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com)... 52.218.229.193\n",
      "Connecting to sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com (sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com)|52.218.229.193|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 432828 (423K) [application/zip]\n",
      "Saving to: ‘bank-additional.zip’\n",
      "\n",
      "bank-additional.zip 100%[===================>] 422.68K  1.45MB/s    in 0.3s    \n",
      "\n",
      "2020-04-14 21:21:38 (1.45 MB/s) - ‘bank-additional.zip’ saved [432828/432828]\n",
      "\n",
      "Archive:  bank-additional.zip\n",
      "   creating: bank-additional/\n",
      "  inflating: bank-additional/bank-additional-names.txt  \n",
      "  inflating: bank-additional/bank-additional.csv  \n",
      "  inflating: bank-additional/bank-additional-full.csv  \n"
     ]
    }
   ],
   "source": [
    "!wget https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip\n",
    "!unzip -o bank-additional.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets read this into a Pandas data frame and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.9y</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>139</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>380</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41178</th>\n",
       "      <td>62</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>thu</td>\n",
       "      <td>483</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>success</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.031</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41179</th>\n",
       "      <td>64</td>\n",
       "      <td>retired</td>\n",
       "      <td>divorced</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>151</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41180</th>\n",
       "      <td>36</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>254</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41181</th>\n",
       "      <td>37</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>281</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41182</th>\n",
       "      <td>29</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>single</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>success</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41183</th>\n",
       "      <td>73</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>334</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41184</th>\n",
       "      <td>46</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>383</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41185</th>\n",
       "      <td>56</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>189</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41186</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>442</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41187</th>\n",
       "      <td>74</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>239</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41188 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age          job   marital            education  default housing loan  \\\n",
       "0       56    housemaid   married             basic.4y       no      no   no   \n",
       "1       57     services   married          high.school  unknown      no   no   \n",
       "2       37     services   married          high.school       no     yes   no   \n",
       "3       40       admin.   married             basic.6y       no      no   no   \n",
       "4       56     services   married          high.school       no      no  yes   \n",
       "5       45     services   married             basic.9y  unknown      no   no   \n",
       "6       59       admin.   married  professional.course       no      no   no   \n",
       "7       41  blue-collar   married              unknown  unknown      no   no   \n",
       "8       24   technician    single  professional.course       no     yes   no   \n",
       "9       25     services    single          high.school       no     yes   no   \n",
       "...    ...          ...       ...                  ...      ...     ...  ...   \n",
       "41178   62      retired   married    university.degree       no      no   no   \n",
       "41179   64      retired  divorced  professional.course       no     yes   no   \n",
       "41180   36       admin.   married    university.degree       no      no   no   \n",
       "41181   37       admin.   married    university.degree       no     yes   no   \n",
       "41182   29   unemployed    single             basic.4y       no     yes   no   \n",
       "41183   73      retired   married  professional.course       no     yes   no   \n",
       "41184   46  blue-collar   married  professional.course       no      no   no   \n",
       "41185   56      retired   married    university.degree       no     yes   no   \n",
       "41186   44   technician   married  professional.course       no      no   no   \n",
       "41187   74      retired   married  professional.course       no     yes   no   \n",
       "\n",
       "         contact month day_of_week  duration  campaign  pdays  previous  \\\n",
       "0      telephone   may         mon       261         1    999         0   \n",
       "1      telephone   may         mon       149         1    999         0   \n",
       "2      telephone   may         mon       226         1    999         0   \n",
       "3      telephone   may         mon       151         1    999         0   \n",
       "4      telephone   may         mon       307         1    999         0   \n",
       "5      telephone   may         mon       198         1    999         0   \n",
       "6      telephone   may         mon       139         1    999         0   \n",
       "7      telephone   may         mon       217         1    999         0   \n",
       "8      telephone   may         mon       380         1    999         0   \n",
       "9      telephone   may         mon        50         1    999         0   \n",
       "...          ...   ...         ...       ...       ...    ...       ...   \n",
       "41178   cellular   nov         thu       483         2      6         3   \n",
       "41179   cellular   nov         fri       151         3    999         0   \n",
       "41180   cellular   nov         fri       254         2    999         0   \n",
       "41181   cellular   nov         fri       281         1    999         0   \n",
       "41182   cellular   nov         fri       112         1      9         1   \n",
       "41183   cellular   nov         fri       334         1    999         0   \n",
       "41184   cellular   nov         fri       383         1    999         0   \n",
       "41185   cellular   nov         fri       189         2    999         0   \n",
       "41186   cellular   nov         fri       442         1    999         0   \n",
       "41187   cellular   nov         fri       239         3    999         1   \n",
       "\n",
       "          poutcome  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  \\\n",
       "0      nonexistent           1.1          93.994          -36.4      4.857   \n",
       "1      nonexistent           1.1          93.994          -36.4      4.857   \n",
       "2      nonexistent           1.1          93.994          -36.4      4.857   \n",
       "3      nonexistent           1.1          93.994          -36.4      4.857   \n",
       "4      nonexistent           1.1          93.994          -36.4      4.857   \n",
       "5      nonexistent           1.1          93.994          -36.4      4.857   \n",
       "6      nonexistent           1.1          93.994          -36.4      4.857   \n",
       "7      nonexistent           1.1          93.994          -36.4      4.857   \n",
       "8      nonexistent           1.1          93.994          -36.4      4.857   \n",
       "9      nonexistent           1.1          93.994          -36.4      4.857   \n",
       "...            ...           ...             ...            ...        ...   \n",
       "41178      success          -1.1          94.767          -50.8      1.031   \n",
       "41179  nonexistent          -1.1          94.767          -50.8      1.028   \n",
       "41180  nonexistent          -1.1          94.767          -50.8      1.028   \n",
       "41181  nonexistent          -1.1          94.767          -50.8      1.028   \n",
       "41182      success          -1.1          94.767          -50.8      1.028   \n",
       "41183  nonexistent          -1.1          94.767          -50.8      1.028   \n",
       "41184  nonexistent          -1.1          94.767          -50.8      1.028   \n",
       "41185  nonexistent          -1.1          94.767          -50.8      1.028   \n",
       "41186  nonexistent          -1.1          94.767          -50.8      1.028   \n",
       "41187      failure          -1.1          94.767          -50.8      1.028   \n",
       "\n",
       "       nr.employed    y  \n",
       "0           5191.0   no  \n",
       "1           5191.0   no  \n",
       "2           5191.0   no  \n",
       "3           5191.0   no  \n",
       "4           5191.0   no  \n",
       "5           5191.0   no  \n",
       "6           5191.0   no  \n",
       "7           5191.0   no  \n",
       "8           5191.0   no  \n",
       "9           5191.0   no  \n",
       "...            ...  ...  \n",
       "41178       4963.6  yes  \n",
       "41179       4963.6   no  \n",
       "41180       4963.6   no  \n",
       "41181       4963.6  yes  \n",
       "41182       4963.6   no  \n",
       "41183       4963.6  yes  \n",
       "41184       4963.6   no  \n",
       "41185       4963.6   no  \n",
       "41186       4963.6  yes  \n",
       "41187       4963.6   no  \n",
       "\n",
       "[41188 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./bank-additional/bank-additional-full.csv')\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 20)         # Keep the output on one page\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about the data.  At a high level, we can see:\n",
    "\n",
    "* We have a little over 40K customer records, and 20 features for each customer\n",
    "* The features are mixed; some numeric, some categorical\n",
    "* The data appears to be sorted, at least by `time` and `contact`, maybe more\n",
    "\n",
    "_**Specifics on each of the features:**_\n",
    "\n",
    "*Demographics:*\n",
    "* `age`: Customer's age (numeric)\n",
    "* `job`: Type of job (categorical: 'admin.', 'services', ...)\n",
    "* `marital`: Marital status (categorical: 'married', 'single', ...)\n",
    "* `education`: Level of education (categorical: 'basic.4y', 'high.school', ...)\n",
    "\n",
    "*Past customer events:*\n",
    "* `default`: Has credit in default? (categorical: 'no', 'unknown', ...)\n",
    "* `housing`: Has housing loan? (categorical: 'no', 'yes', ...)\n",
    "* `loan`: Has personal loan? (categorical: 'no', 'yes', ...)\n",
    "\n",
    "*Past direct marketing contacts:*\n",
    "* `contact`: Contact communication type (categorical: 'cellular', 'telephone', ...)\n",
    "* `month`: Last contact month of year (categorical: 'may', 'nov', ...)\n",
    "* `day_of_week`: Last contact day of the week (categorical: 'mon', 'fri', ...)\n",
    "* `duration`: Last contact duration, in seconds (numeric). Important note: If duration = 0 then `y` = 'no'.\n",
    " \n",
    "*Campaign information:*\n",
    "* `campaign`: Number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "* `pdays`: Number of days that passed by after the client was last contacted from a previous campaign (numeric)\n",
    "* `previous`: Number of contacts performed before this campaign and for this client (numeric)\n",
    "* `poutcome`: Outcome of the previous marketing campaign (categorical: 'nonexistent','success', ...)\n",
    "\n",
    "*External environment factors:*\n",
    "* `emp.var.rate`: Employment variation rate - quarterly indicator (numeric)\n",
    "* `cons.price.idx`: Consumer price index - monthly indicator (numeric)\n",
    "* `cons.conf.idx`: Consumer confidence index - monthly indicator (numeric)\n",
    "* `euribor3m`: Euribor 3 month rate - daily indicator (numeric)\n",
    "* `nr.employed`: Number of employees - quarterly indicator (numeric)\n",
    "\n",
    "*Target variable:*\n",
    "* `y`: Has the client subscribed a term deposit? (binary: 'yes','no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "Let's start exploring the data.  First, let's understand how the features are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Frequency tables for each categorical feature\n",
    "#for column in data.select_dtypes(include=['object']).columns:\n",
    "#    display(pd.crosstab(index=data[column], columns='% observations', normalize='columns'))\n",
    "\n",
    "# Histograms for each numeric features\n",
    "#display(data.describe())\n",
    "#%matplotlib inline\n",
    "#hist = data.hist(bins=30, sharey=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "\n",
    "* Almost 90% of the values for our target variable `y` are \"no\", so most customers did not subscribe to a term deposit.\n",
    "* Many of the predictive features take on values of \"unknown\".  Some are more common than others.  We should think carefully as to what causes a value of \"unknown\" (are these customers non-representative in some way?) and how we that should be handled.\n",
    "  * Even if \"unknown\" is included as it's own distinct category, what does it mean given that, in reality, those observations likely fall within one of the other categories of that feature?\n",
    "* Many of the predictive features have categories with very few observations in them.  If we find a small category to be highly predictive of our target outcome, do we have enough evidence to make a generalization about that?\n",
    "* Contact timing is particularly skewed.  Almost a third in May and less than 1% in December.  What does this mean for predicting our target variable next December?\n",
    "* There are no missing values in our numeric features.  Or missing values have already been imputed.\n",
    "  * `pdays` takes a value near 1000 for almost all customers.  Likely a placeholder value signifying no previous contact.\n",
    "* Several numeric features have a very long tail.  Do we need to handle these few observations with extremely large values differently?\n",
    "* Several numeric features (particularly the macroeconomic ones) occur in distinct buckets.  Should these be treated as categorical?\n",
    "\n",
    "Next, let's look at how our features relate to the target that we are attempting to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for column in data.select_dtypes(include=['object']).columns:\n",
    "#    if column != 'y':\n",
    "#        display(pd.crosstab(index=data[column], columns=data['y'], normalize='columns'))\n",
    "\n",
    "#for column in data.select_dtypes(exclude=['object']).columns:\n",
    "#    print(column)\n",
    "#    hist = data[[column, 'y']].hist(by='y', bins=30)\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "\n",
    "* Customers who are-- \"blue-collar\", \"married\", \"unknown\" default status, contacted by \"telephone\", and/or in \"may\" are a substantially lower portion of \"yes\" than \"no\" for subscribing.\n",
    "* Distributions for numeric variables are different across \"yes\" and \"no\" subscribing groups, but the relationships may not be straightforward or obvious.\n",
    "\n",
    "Now let's look at how our features relate to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(data.corr())\n",
    "#pd.plotting.scatter_matrix(data, figsize=(12, 12))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "* Features vary widely in their relationship with one another.  Some with highly negative correlation, others with highly positive correlation.\n",
    "* Relationships between features is non-linear and discrete in many cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation\n",
    "\n",
    "Cleaning up data is part of nearly every machine learning project.  It arguably presents the biggest risk if done incorrectly and is one of the more subjective aspects in the process.  Several common techniques include:\n",
    "\n",
    "* Handling missing values: Some machine learning algorithms are capable of handling missing values, but most would rather not.  Options include:\n",
    " * Removing observations with missing values: This works well if only a very small fraction of observations have incomplete information.\n",
    " * Removing features with missing values: This works well if there are a small number of features which have a large number of missing values.\n",
    " * Imputing missing values: Entire [books](https://www.amazon.com/Flexible-Imputation-Missing-Interdisciplinary-Statistics/dp/1439868247) have been written on this topic, but common choices are replacing the missing value with the mode or mean of that column's non-missing values.\n",
    "* Converting categorical to numeric: The most common method is one hot encoding, which for each feature maps every distinct value of that column to its own feature which takes a value of 1 when the categorical feature is equal to that value, and 0 otherwise.\n",
    "* Oddly distributed data: Although for non-linear models like Gradient Boosted Trees, this has very limited implications, parametric models like regression can produce wildly inaccurate estimates when fed highly skewed data.  In some cases, simply taking the natural log of the features is sufficient to produce more normally distributed data.  In others, bucketing values into discrete ranges is helpful.  These buckets can then be treated as categorical variables and included in the model when one hot encoded.\n",
    "* Handling more complicated data types: Mainpulating images, text, or data at varying grains is left for other notebook templates.\n",
    "\n",
    "Luckily, some of these aspects have already been handled for us, and the algorithm we are showcasing tends to do well at handling sparse or oddly distributed data.  Therefore, let's keep pre-processing simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['no_previous_contact'] = np.where(data['pdays'] == 999, 1, 0)                                 # Indicator variable to capture when pdays takes a value of 999\n",
    "data['not_working'] = np.where(np.in1d(data['job'], ['student', 'retired', 'unemployed']), 1, 0)   # Indicator for individuals not actively employed\n",
    "model_data = pd.get_dummies(data)                                                                  # Convert categorical variables to sets of indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another question to ask yourself before building a model is whether certain features will add value in your final use case.  For example, if your goal is to deliver the best prediction, then will you have access to that data at the moment of prediction?  Knowing it's raining is highly predictive for umbrella sales, but forecasting weather far enough out to plan inventory on umbrellas is probably just as difficult as forecasting umbrella sales without knowledge of the weather.  So, including this in your model may give you a false sense of precision.\n",
    "\n",
    "Following this logic, let's remove the economic features and `duration` from our data as they would need to be forecasted with high precision to use as inputs in future predictions.\n",
    "\n",
    "Even if we were to use values of the economic indicators from the previous quarter, this value is likely not as relevant for prospects contacted early in the next quarter as those contacted later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data.drop(['duration', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a model whose primary goal is to predict a target value on new data, it is important to understand overfitting.  Supervised learning models are designed to minimize error between their predictions of the target value and actuals, in the data they are given.  This last part is key, as frequently in their quest for greater accuracy, machine learning models bias themselves toward picking up on minor idiosyncrasies within the data they are shown.  These idiosyncrasies then don't repeat themselves in subsequent data, meaning those predictions can actually be made less accurate, at the expense of more accurate predictions in the training phase.\n",
    "\n",
    "The most common way of preventing this is to build models with the concept that a model shouldn't only be judged on its fit to the data it was trained on, but also on \"new\" data.  There are several different ways of operationalizing this, holdout validation, cross-validation, leave-one-out validation, etc.  For our purposes, we'll simply randomly split the data into 3 uneven groups.  The model will be trained on 70% of data, it will then be evaluated on 20% of data to give us an estimate of the accuracy we hope to have on \"new\" data, and 10% will be held back as a final testing dataset which will be used later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])   # Randomly sort the data then split out first 70%, second 20%, and last 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker's XGBoost container expects data in the libSVM or CSV data format.  For this example, we'll stick to CSV.  Note that the first column must be the target variable and the CSV should not include headers.  Also, notice that although repetitive it's easiest to do this after the train|validation|test split rather than before.  This avoids any misalignment issues due to random reordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "pd.concat([validation_data['y_yes'], validation_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll copy the file to S3 for Amazon SageMaker's managed training to pickup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training\n",
    "Now we know that most of our features have skewed distributions, some are highly correlated with one another, and some appear to have non-linear relationships with our target variable.  Also, for targeting future prospects, good predictive accuracy is preferred to being able to explain why that prospect was targeted.  Taken together, these aspects make gradient boosted trees a good candidate algorithm.\n",
    "\n",
    "There are several intricacies to understanding the algorithm, but at a high level, gradient boosted trees works by combining predictions from many simple models, each of which tries to address the weaknesses of the previous models.  By doing this the collection of simple models can actually outperform large, complex models.  Other Amazon SageMaker notebooks elaborate on gradient boosting trees further and how they differ from similar algorithms.\n",
    "\n",
    "`xgboost` is an extremely popular, open-source package for gradient boosted trees.  It is computationally powerful, fully featured, and has been successfully used in many machine learning competitions.  Let's start with a simple `xgboost` model, trained using Amazon SageMaker's managed, distributed training framework.\n",
    "\n",
    "First we'll need to specify the ECR container location for Amazon SageMaker's implementation of XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='0.90-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '0.90-1').\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create `s3_input`s that our training function can use as a pointer to the files in S3, which also specify that the content type is CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll need to specify training parameters to the estimator.  This includes:\n",
    "1. The `xgboost` algorithm container\n",
    "1. The IAM role to use\n",
    "1. Training instance type and count\n",
    "1. S3 location for output data\n",
    "1. Algorithm hyperparameters\n",
    "\n",
    "And then a `.fit()` function which specifies:\n",
    "1. S3 location for output data.  In this case we have both a training and validation set which are passed in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Console\n",
    "After running the command below, go to SageMaker `Training Jobs` menu and observe the training job initiated.  Discussion points with your instructor:\n",
    "\n",
    " - View status\n",
    " - Check `Training duration`  (training takes approx 3 minutes to complete)\n",
    " - Enter Job details\n",
    " - Check `Billable time (seconds)` (billable time was 55 seconds)\n",
    " - Observe `Channels`\n",
    "    - What are they? (Train and Test channels)\n",
    " - Identify `S3 distribution type` (Fully replicated means, all instances get the full dataset, when a cluster is used for training`\n",
    " - Observe metrics\n",
    "   - Built-in algorithms capture and emit many metrics to evaluate your model.\n",
    "   - Observe how metrics are persisted in CloudWatch\n",
    "   - Observe how EC2 metrics are also persisted in CloudWatch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-14 21:35:59 Starting - Starting the training job...\n",
      "2020-04-14 21:36:01 Starting - Launching requested ML instances.........\n",
      "2020-04-14 21:37:46 Starting - Preparing the instances for training......\n",
      "2020-04-14 21:38:44 Downloading - Downloading input data...\n",
      "2020-04-14 21:39:15 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2020-04-14:21:39:35:INFO] Running distributed xgboost training.\u001b[0m\n",
      "\u001b[34m[2020-04-14:21:39:38:INFO] Number of hosts: 2, master IP address: 10.2.128.54, host IP address: 10.2.128.54.\u001b[0m\n",
      "\u001b[34m[2020-04-14:21:39:38:INFO] Finished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\u001b[35mArguments: train\u001b[0m\n",
      "\u001b[35m[2020-04-14:21:39:36:INFO] Running distributed xgboost training.\u001b[0m\n",
      "\u001b[35m[2020-04-14:21:39:37:INFO] Number of hosts: 2, master IP address: 10.2.128.54, host IP address: 10.2.190.229.\u001b[0m\n",
      "\u001b[35m[2020-04-14:21:39:37:INFO] Finished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\u001b[35mstarting datanode, logging to /opt/amazon/hadoop/logs/hadoop--datanode-ip-10-2-190-229.ec2.internal.out\u001b[0m\n",
      "\u001b[34mstarting namenode, logging to /opt/amazon/hadoop/logs/hadoop--namenode-ip-10-2-128-54.ec2.internal.out\u001b[0m\n",
      "\u001b[35mstarting nodemanager, logging to /opt/amazon/hadoop/logs/yarn--nodemanager-ip-10-2-190-229.ec2.internal.out\u001b[0m\n",
      "\u001b[35m[2020-04-14:21:39:42:INFO] File size need to be processed in the node: 2.17mb. Available memory size in the node: 8258.68mb\u001b[0m\n",
      "\u001b[34mstarting resourcemanager, logging to /opt/amazon/hadoop/logs/yarn--resourcemanager-ip-10-2-128-54.ec2.internal.out\u001b[0m\n",
      "\u001b[34mstarting datanode, logging to /opt/amazon/hadoop/logs/hadoop--datanode-ip-10-2-128-54.ec2.internal.out\u001b[0m\n",
      "\n",
      "2020-04-14 21:39:35 Training - Training image download completed. Training in progress.\u001b[34mstarting nodemanager, logging to /opt/amazon/hadoop/logs/yarn--nodemanager-ip-10-2-128-54.ec2.internal.out\u001b[0m\n",
      "\u001b[34m[2020-04-14:21:39:50:INFO] File size need to be processed in the node: 2.17mb. Available memory size in the node: 7779.29mb\u001b[0m\n",
      "\u001b[34m[2020-04-14:21:39:50:INFO] HTTP server started....\u001b[0m\n",
      "\u001b[34m[2020-04-14:21:39:50:INFO] Memory/core ratio is 7.83\u001b[0m\n",
      "\u001b[34m[2020-04-14:21:39:50:INFO] Yarn setup: number of workers: 2, physical cores per worker: 2, physical memory per worker: 15.67g.\u001b[0m\n",
      "\u001b[34m[2020-04-14:21:39:50:INFO] Yarn job submitted successfully.\u001b[0m\n",
      "\u001b[34m2020-04-14 21:39:51,001 INFO start listen on 10.2.128.54:9091\u001b[0m\n",
      "\u001b[34m/xgboost/dmlc-core/tracker/dmlc_tracker/yarn.py:37: UserWarning: cannot find \"/xgboost/dmlc-core/tracker/dmlc_tracker/../yarn/dmlc-yarn.jar\", I will try to run build\n",
      "  warnings.warn(\"cannot find \\\"%s\\\", I will try to run build\" % YARN_JAR_PATH)\u001b[0m\n",
      "\u001b[34msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:37: warning: Signal is internal proprietary API and may be removed in a future release\u001b[0m\n",
      "\u001b[34mimport sun.misc.Signal;\n",
      "               ^\u001b[0m\n",
      "\u001b[34msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:38: warning: SignalHandler is internal proprietary API and may be removed in a future release\u001b[0m\n",
      "\u001b[34mimport sun.misc.SignalHandler;\n",
      "               ^\u001b[0m\n",
      "\u001b[34msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:276: warning: Signal is internal proprietary API and may be removed in a future release\n",
      "        Signal intSignal = new Signal(\"INT\");\n",
      "        ^\u001b[0m\n",
      "\u001b[34msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:276: warning: Signal is internal proprietary API and may be removed in a future release\n",
      "        Signal intSignal = new Signal(\"INT\");\n",
      "                               ^\u001b[0m\n",
      "\u001b[34msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:277: warning: Signal is internal proprietary API and may be removed in a future release\n",
      "        Signal.handle(intSignal, handler);\n",
      "        ^\u001b[0m\n",
      "\u001b[34msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:332: warning: SignalHandler is internal proprietary API and may be removed in a future release\n",
      "    class CtrlCHandler implements SignalHandler{\n",
      "                                  ^\u001b[0m\n",
      "\u001b[34msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:339: warning: Signal is internal proprietary API and may be removed in a future release\n",
      "        public void handle(Signal signal){\n",
      "                           ^\u001b[0m\n",
      "\u001b[34mNote: src/main/java/org/apache/hadoop/yarn/dmlc/ApplicationMaster.java uses unchecked or unsafe operations.\u001b[0m\n",
      "\u001b[34mNote: Recompile with -Xlint:unchecked for details.\u001b[0m\n",
      "\u001b[34m7 warnings\u001b[0m\n",
      "\u001b[34m20/04/14 21:39:55 INFO client.RMProxy: Connecting to ResourceManager at algo-1/10.2.128.54:8032\u001b[0m\n",
      "\u001b[34m20/04/14 21:39:56 INFO dmlc.Client: HDFS temp directory do not exist, creating.. /tmp\u001b[0m\n",
      "\u001b[34m20/04/14 21:39:57 INFO dmlc.Client: jobname=DMLC[nworker=2]:python,username=root\u001b[0m\n",
      "\u001b[34m20/04/14 21:39:57 INFO dmlc.Client: Submitting application application_1586900386389_0001\u001b[0m\n",
      "\u001b[34m20/04/14 21:39:57 INFO impl.YarnClientImpl: Submitted application application_1586900386389_0001\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:05,242 INFO @tracker All of 2 nodes getting started\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:05,542 INFO [0]#011train-error:0.100343#011validation-error:0.103059\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:05,765 INFO [1]#011train-error:0.099233#011validation-error:0.103302\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:05,985 INFO [2]#011train-error:0.099476#011validation-error:0.10318\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:06,201 INFO [3]#011train-error:0.099892#011validation-error:0.102573\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:06,417 INFO [4]#011train-error:0.099788#011validation-error:0.102816\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:06,628 INFO [5]#011train-error:0.099719#011validation-error:0.102573\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:06,841 INFO [6]#011train-error:0.099719#011validation-error:0.102331\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:07,071 INFO [7]#011train-error:0.099684#011validation-error:0.102452\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:07,291 INFO [8]#011train-error:0.099684#011validation-error:0.102331\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:07,505 INFO [9]#011train-error:0.099233#011validation-error:0.102573\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:07,717 INFO [10]#011train-error:0.099129#011validation-error:0.102573\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:07,928 INFO [11]#011train-error:0.098783#011validation-error:0.102452\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:08,141 INFO [12]#011train-error:0.098713#011validation-error:0.102573\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:08,361 INFO [13]#011train-error:0.099095#011validation-error:0.102452\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:08,576 INFO [14]#011train-error:0.098713#011validation-error:0.102452\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:08,833 INFO [15]#011train-error:0.098887#011validation-error:0.102938\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:09,049 INFO [16]#011train-error:0.098401#011validation-error:0.103302\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:09,257 INFO [17]#011train-error:0.098262#011validation-error:0.103302\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:09,481 INFO [18]#011train-error:0.098193#011validation-error:0.103423\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:09,693 INFO [19]#011train-error:0.098262#011validation-error:0.103909\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:09,905 INFO [20]#011train-error:0.098193#011validation-error:0.103787\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:10,121 INFO [21]#011train-error:0.097985#011validation-error:0.10403\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:10,337 INFO [22]#011train-error:0.098089#011validation-error:0.10403\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:10,549 INFO [23]#011train-error:0.098054#011validation-error:0.103423\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:10,752 INFO [24]#011train-error:0.098228#011validation-error:0.103909\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:10,956 INFO [25]#011train-error:0.097846#011validation-error:0.103666\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:11,161 INFO [26]#011train-error:0.097673#011validation-error:0.103787\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:11,365 INFO [27]#011train-error:0.097742#011validation-error:0.103787\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:11,572 INFO [28]#011train-error:0.097673#011validation-error:0.10403\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:11,773 INFO [29]#011train-error:0.097673#011validation-error:0.104151\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:11,976 INFO [30]#011train-error:0.097326#011validation-error:0.10488\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:12,181 INFO [31]#011train-error:0.097152#011validation-error:0.104758\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:12,389 INFO [32]#011train-error:0.097187#011validation-error:0.104394\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:12,601 INFO [33]#011train-error:0.097048#011validation-error:0.104273\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:12,804 INFO [34]#011train-error:0.097014#011validation-error:0.104394\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:13,008 INFO [35]#011train-error:0.097014#011validation-error:0.104394\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:13,209 INFO [36]#011train-error:0.097048#011validation-error:0.104516\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:13,429 INFO [37]#011train-error:0.09691#011validation-error:0.104273\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:13,641 INFO [38]#011train-error:0.097048#011validation-error:0.104394\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:13,845 INFO [39]#011train-error:0.09691#011validation-error:0.10403\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:14,045 INFO [40]#011train-error:0.096806#011validation-error:0.104273\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:14,248 INFO [41]#011train-error:0.097152#011validation-error:0.10403\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:14,457 INFO [42]#011train-error:0.096944#011validation-error:0.104516\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:14,661 INFO [43]#011train-error:0.096979#011validation-error:0.104516\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:14,861 INFO [44]#011train-error:0.096736#011validation-error:0.104516\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:15,065 INFO [45]#011train-error:0.096459#011validation-error:0.104394\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:15,269 INFO [46]#011train-error:0.096528#011validation-error:0.104637\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:15,485 INFO [47]#011train-error:0.096563#011validation-error:0.104394\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:15,693 INFO [48]#011train-error:0.096493#011validation-error:0.104394\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:15,901 INFO [49]#011train-error:0.096493#011validation-error:0.10488\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:16,105 INFO [50]#011train-error:0.096667#011validation-error:0.105244\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:16,309 INFO [51]#011train-error:0.096701#011validation-error:0.104758\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:16,513 INFO [52]#011train-error:0.096597#011validation-error:0.104637\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:16,721 INFO [53]#011train-error:0.096632#011validation-error:0.104637\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:16,933 INFO [54]#011train-error:0.096563#011validation-error:0.104516\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:17,136 INFO [55]#011train-error:0.096493#011validation-error:0.104637\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:17,353 INFO [56]#011train-error:0.096667#011validation-error:0.104637\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:17,556 INFO [57]#011train-error:0.096493#011validation-error:0.104637\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:17,761 INFO [58]#011train-error:0.096389#011validation-error:0.10488\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:17,965 INFO [59]#011train-error:0.096459#011validation-error:0.105123\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:18,181 INFO [60]#011train-error:0.096493#011validation-error:0.105001\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:18,385 INFO [61]#011train-error:0.09684#011validation-error:0.10488\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:18,597 INFO [62]#011train-error:0.096701#011validation-error:0.105001\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:18,808 INFO [63]#011train-error:0.096701#011validation-error:0.105001\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:19,013 INFO [64]#011train-error:0.096493#011validation-error:0.10488\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:19,217 INFO [65]#011train-error:0.09632#011validation-error:0.105123\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-04-14 21:40:19,421 INFO [66]#011train-error:0.096424#011validation-error:0.105001\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:19,629 INFO [67]#011train-error:0.096216#011validation-error:0.104758\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:19,833 INFO [68]#011train-error:0.096112#011validation-error:0.104637\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:20,045 INFO [69]#011train-error:0.096251#011validation-error:0.104516\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:20,257 INFO [70]#011train-error:0.096216#011validation-error:0.104637\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:20,473 INFO [71]#011train-error:0.096008#011validation-error:0.104637\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:20,676 INFO [72]#011train-error:0.096077#011validation-error:0.104516\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:20,885 INFO [73]#011train-error:0.096008#011validation-error:0.104394\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:21,089 INFO [74]#011train-error:0.096112#011validation-error:0.104394\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:21,301 INFO [75]#011train-error:0.096042#011validation-error:0.104394\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:21,514 INFO [76]#011train-error:0.095869#011validation-error:0.104273\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:21,729 INFO [77]#011train-error:0.095834#011validation-error:0.104273\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:21,933 INFO [78]#011train-error:0.09573#011validation-error:0.104758\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:22,137 INFO [79]#011train-error:0.0958#011validation-error:0.104394\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:22,353 INFO [80]#011train-error:0.09573#011validation-error:0.104637\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:22,557 INFO [81]#011train-error:0.095661#011validation-error:0.10488\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:22,761 INFO [82]#011train-error:0.095626#011validation-error:0.105123\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:22,965 INFO [83]#011train-error:0.095626#011validation-error:0.104637\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:23,169 INFO [84]#011train-error:0.095522#011validation-error:0.104758\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:23,373 INFO [85]#011train-error:0.095453#011validation-error:0.104516\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:23,577 INFO [86]#011train-error:0.095418#011validation-error:0.104758\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:23,781 INFO [87]#011train-error:0.095314#011validation-error:0.104637\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:23,985 INFO [88]#011train-error:0.095349#011validation-error:0.10488\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:24,193 INFO [89]#011train-error:0.09521#011validation-error:0.10488\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:24,413 INFO [90]#011train-error:0.095175#011validation-error:0.10488\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:24,633 INFO [91]#011train-error:0.095106#011validation-error:0.10488\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:24,837 INFO [92]#011train-error:0.095106#011validation-error:0.104758\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:25,037 INFO [93]#011train-error:0.095106#011validation-error:0.10488\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:25,245 INFO [94]#011train-error:0.094967#011validation-error:0.10488\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:25,449 INFO [95]#011train-error:0.094863#011validation-error:0.10488\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:25,653 INFO [96]#011train-error:0.094933#011validation-error:0.104758\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:25,857 INFO [97]#011train-error:0.094898#011validation-error:0.105001\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:26,061 INFO [98]#011train-error:0.094898#011validation-error:0.10488\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:26,267 INFO [99]#011train-error:0.094933#011validation-error:0.104637\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:26,271 INFO Finished training\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:26,273 INFO Finished training\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:26,273 INFO @tracker All nodes finishes job\u001b[0m\n",
      "\u001b[34m2020-04-14 21:40:26,273 INFO @tracker 21.0306739807 secs between node start and job finish\u001b[0m\n",
      "\n",
      "2020-04-14 21:41:56 Uploading - Uploading generated training model\n",
      "2020-04-14 21:41:56 Completed - Training job completed\n",
      "\u001b[35m[2020-04-14:21:41:47:INFO] Master host is not alive. Training might have finished. Shutting down.... Check the logs for algo-1 machine.\u001b[0m\n",
      "Training seconds: 384\n",
      "Billable seconds: 384\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=2, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Algorithm Overview\n",
    "\n",
    "Check Algorithm Parameters & Hyperparameters in SageMaker Dev Guide: https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html\n",
    "\n",
    "Discussion points with your instructor:\n",
    "\n",
    "- Observe how it is easy to prepare a training instance for XGBoost algorithm (You just specify a container image)\n",
    "- Observe how it is easy to to clustered ML training in the cloud (You just specify the number of instances. The data is provided to each instance via S3 implicitly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Hyperparameters\n",
    "\n",
    "Understand the `Objective` Hyperparameter. \n",
    "\n",
    "Check: DMLC web site for XGBoost: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst\n",
    "            \n",
    "`\n",
    "Learning Task Parameters\n",
    "Specify the learning task and the corresponding learning objective. The objective options are below:\n",
    "\n",
    "Some objectives:\n",
    "\n",
    "    - reg:squarederror: regression with squared loss.\n",
    "\n",
    "    - binary:logistic: logistic regression for binary classification, output probability\n",
    "\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Training Job Overview\n",
    "After running the command below, go to SageMaker `Training Jobs` menu and observe the training job initiated.  Discussion points with your instructor:\n",
    "\n",
    " - View status\n",
    " - Check `Training duration`  (training takes approx 3 minutes to complete)\n",
    " - Enter Job details\n",
    " - Check `Billable time (seconds)` (billable time was 55 seconds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Data Set Overview\n",
    "- Observe `Channels`\n",
    "    - What are they? (Train and Test channels)\n",
    " - Identify `S3 distribution type` (Fully replicated means, all instances get the full dataset, when a cluster is used for training`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Training Monitoring\n",
    "- Observe metrics\n",
    "   - Built-in algorithms capture and emit many metrics to evaluate your model.\n",
    "   - Observe how metrics are persisted in CloudWatch\n",
    "   - Observe how EC2 metrics are also persisted in CloudWatch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations\n",
    "[TBD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hosting\n",
    "Now that we've trained the `xgboost` algorithm on our data, let's deploy a model that's hosted behind a real-time endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_predictor = xgb.deploy(initial_instance_count=1,\n",
    "#                           instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluation\n",
    "There are many ways to compare the performance of a machine learning model, but let's start by simply comparing actual to predicted values.  In this case, we're simply predicting whether the customer subscribed to a term deposit (`1`) or not (`0`), which produces a simple confusion matrix.\n",
    "\n",
    "First we'll need to determine how we pass data into and receive data from our endpoint.  Our data is currently stored as NumPy arrays in memory of our notebook instance.  To send it in an HTTP POST request, we'll serialize it as a CSV string and then decode the resulting CSV.\n",
    "\n",
    "*Note: For inference with CSV format, SageMaker XGBoost requires that the data does NOT include the target variable.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_predictor.content_type = 'text/csv'\n",
    "#xgb_predictor.serializer = csv_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a simple function to:\n",
    "1. Loop over our test dataset\n",
    "1. Split it into mini-batches of rows \n",
    "1. Convert those mini-batches to CSV string payloads (notice, we drop the target variable from our dataset first)\n",
    "1. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "1. Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def predict(data, rows=500):\n",
    "#    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "#    predictions = ''\n",
    "#    for array in split_array:\n",
    "#        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "#    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "#predictions = predict(test_data.drop(['y_no', 'y_yes'], axis=1).as_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll check our confusion matrix to see how well we predicted versus actuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.crosstab(index=test_data['y_yes'], columns=np.round(predictions), rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, of the ~4000 potential customers, we predicted 136 would subscribe and 94 of them actually did.  We also had 389 subscribers who subscribed that we did not predict would.  This is less than desirable, but the model can (and should) be tuned to improve this.  Most importantly, note that with minimal effort, our model produced accuracies similar to those published [here](http://media.salford-systems.com/video/tutorial/2015/targeted_marketing.pdf).\n",
    "\n",
    "_Note that because there is some element of randomness in the algorithm's subsample, your results may differ slightly from the text written above._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extensions\n",
    "\n",
    "This example analyzed a relatively small dataset, but utilized Amazon SageMaker features such as distributed, managed training and real-time model hosting, which could easily be applied to much larger problems.  In order to improve predictive accuracy further, we could tweak value we threshold our predictions at to alter the mix of false-positives and false-negatives, or we could explore techniques like hyperparameter tuning.  In a real-world scenario, we would also spend more time engineering features by hand and would likely look for additional datasets to include which contain customer information not available in our initial dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Clean-up\n",
    "\n",
    "If you are done with this notebook, please run the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
